# Interview Preparation: AI & Robotics Web Dashboard

이 문서는 프로젝트 포트폴리오 면접을 대비하여 예상 질문, 답변, 그리고 기술적 배경 지식을 정리한 자료입니다.

## 1. Project Overview (프로젝트 개요)

**프로젝트 명:** AI 기반 터틀봇3 웹 관제 대시보드 (AI-Powered TurtleBot3 Web Dashboard)

**핵심 기능:**
*   **원격 제어:** 웹 인터페이스를 통해 ROS2 기반의 터틀봇3를 실시간으로 조종.
*   **객체 인식 (Surveillance):** YOLOv8/v11 모델을 활용하여 실시간으로 사람 및 특정 객체를 인식하고 감시 모드 작동.
*   **AI 챗봇 (PHi-4):** 로봇의 상태를 자연어로 물어보거나 웹 제어를 할 수 있는 LLM 탑재 (RAG/Context Memory 포함).
*   **자율 주행 (Nav2):** SLAM으로 생성된 지도를 웹 캔버스에 시각화하고, 클릭으로 목적지를 설정하여 자율 주행 명령.

**기술 스택 (Tech Stack):**
*   **Frontend:** HTML5, CSS3 (Modern/Glassmorphism), JavaScript (Vanilla), Jinja2 Templates
*   **Backend:** Python (FastAPI), AsyncIO
*   **Robotics:** ROS2 (Humble), Nav2 (Navigation2), Rosbridge (WebSocket)
*   **AI/ML:** YOLO (Object Detection), PHi-4 (On-device LLM/Transformers), PyTorch
*   **Infra/Tools:** Docker (optional), Linux (Ubuntu), Git

---

## 2. Expected Interview Questions & Answers (예상 면접 질문 및 답변)

### Q1. 시스템 아키텍처에 대해 설명해 주세요. (System Architecture)
**질문 의도:** 전체적인 데이터 흐름과 서로 다른 시스템(웹, 로봇, AI)이 어떻게 통신하는지 이해하고 있는지 확인.

**모범 답안 (사용자 프로젝트 맞춤형):**
"이 프로젝트는 **FastAPI 웹 서버**, **ROS2 로봇 제어부**, 그리고 **독립된 CCTV 및 DB 서버**로 구성된 분산 시스템입니다.

1.  **웹 & AI (Web & AI):** 사용자가 FastAPI 서버에 접속하면 Index/Dashboard 페이지를 통해 로봇을 제어합니다. 특이한 점은 제가 직접 튜닝한 **PHi-4 기반 챗봇**이 내장되어 있다는 것입니다. 엣지 디바이스의 메모리 한계를 극복하기 위해 모델 양자화(Quantization)와 컨텍스트 최적화를 적용했고, 부족한 지식은 **Google Search API**를 연동한 RAG(검색 증강 생성) 기술로 보완했습니다.

2.  **네트워크 인프라 (Network):** 노트북이 **두 개의 네트워크 인터페이스(123.x 대역과 0.x 대역)**를 동시에 관리하는 것이 핵심 챌린지였습니다.
    *   **123.x 대역:** 터틀봇3와의 실시간 ROS2 통신을 담당합니다.
    *   **0.x 대역:** CCTV 서버와 통신합니다. CCTV 서버는 독립적으로 움직임을 감지하고 소음이 75dB을 넘으면 DB 서버(PostgreSQL)로 데이터를 전송하는 이벤트 기반 아키텍처를 따릅니다.

3.  **로보틱스 기능 (Robotics):**
    *   **순찰 모드:** Nav2를 이용해 자율 순찰을 돌다가 사람을 인식하면 접근합니다.
    *   **펫 모드:** YOLO(비전)와 LiDAR(거리 센서)를 결합한 **센서 퓨전(Sensor Fusion)** 기술을 사용했습니다. 학습된 '주인(Owner)'을 인식하면 라이다로 40cm 거리를 실시간으로 계산하여 일정 간격을 유지하며 추종하도록 구현했습니다."

### Q2. 왜 ROS2를 사용했나요? (Why ROS2?)
**질문 의도:** ROS1과의 차이점, 혹은 다른 통신 방식(Socket 등) 대신 미들웨어를 쓴 이유.

**모범 답안:**
"기존 ROS1은 마스터 노드 의존성이 높아 단일 실패 지점(SPOF) 문제가 있었고, 실시간성(Real-time) 보장이 어려웠습니다.
반면 **ROS2**는 DDS(Data Distribution Service)를 기반으로 하여 노드 간 검색(Discovery)이 자동화되어 있고, 통신 QoS(Quality of Service) 설정을 통해 네트워크가 불안정한 환경에서도 데이터 손실을 관리할 수 있습니다.
또한 Python 3와의 호환성이 완벽하고, 최신 Nav2 스택을 활용하기 위해 ROS2 Humble 버전을 채택했습니다."

### Q3. YOLO 모델을 웹 스트리밍에 적용하면서 겪은 어려움은 무엇인가요? (Challenges with YOLO & Streaming)
**질문 의도:** 성능 최적화 경험, Latency(지연) 해결 능력.

**모범 답안:**
"초기에는 매 프레임마다 YOLO 추론을 돌렸더니 FPS가 낮아지고 웹 브라우저에서 스트림이 끊기는 현상(Latency)이 발생했습니다.
이를 해결하기 위해 두 가지 최적화를 수행했습니다.
첫째, **비동기 스트리밍**입니다. OpenCV의 이미지 인코딩과 웹 전송을 FastAPI의 `StreamingResponse`와 제너레이터 패턴으로 구현하여 메모리 효율을 높였습니다.
둘째, **추론 주기 조절**입니다. 모든 프레임이 아닌, 3~5 프레임마다 한 번씩 추론을 수행하거나 `cv2.dnn` 모듈을 활용한 경량화 기법을 고려하여 실시간성을 확보하면서도 웹 대시보드의 반응성을 유지했습니다."

### Q4. PHi-4 같은 LLM을 로컬에 올린 이유는 무엇이며, 어떻게 최적화했나요? (On-device LLM)
**질문 의도:** 클라우드 API(GPT-4) 대신 로컬 모델을 쓴 이유와 리소스 관리 능력.

**모범 답안:**
"로봇이 감시 모드 등 보안이 중요한 역할을 수행할 수 있기 때문에, 영상 데이터나 민감한 로그가 외부 서버로 나가는 것을 방지하고자 **On-device LLM(PHi-4)**을 선택했습니다.
하지만 PHi-4도 엣지 디바이스에서는 무거울 수 있습니다. 따라서 모델 로딩 시 `torch.float16`이나 `quantization_config`를 활용해 VRAM 사용량을 줄였고, 챗봇 기능이 필요 없을 때는 모델을 언로드(Unload)하여 로봇 주행 리소스를 확보하도록 생명주기를 관리했습니다."

### Q5. Nav2(Navigation2) 스택 설정에서 가장 까다로웠던 점은?
**질문 의도:** 내비게이션 튜닝 경험.

**모범 답안:**
"Costmap(비용 지도) 설정이 가장 까다로웠습니다. 터틀봇3의 크기에 비해 Inflation Radius(장애물 회피 반경)가 너무 크면 좁은 통로를 지나가지 못하고, 너무 작으면 장애물에 부딪히는 문제가 있었습니다.
이를 해결하기 위해 `nav2_params.yaml` 파일에서 `inflation_radius`와 `cost_scaling_factor`를 반복적으로 튜닝하며 최적의 값을 찾았습니다. 또한, 웹 캔버스와 맵 좌표계(Map Frame) 간의 좌표 변환 행렬을 맞추는 작업도 중요했습니다. `roslibjs`의 TFClient를 활용하여 map 좌표계와 base_link 좌표계를 동기화하는 부분에서 많은 디버깅이 필요했습니다."

### Q6. 서로 다른 두 네트워크(123.x, 0.x)를 쓸 때 ROS2 통신 문제는 없었나요?
**질문 의도:** 멀티 네트워크 환경에서 ROS2 DDS 설정 경험 확인.

**모범 답안:**
"네, 처음에는 ROS2 패킷이 0.x 대역으로도 전송되면서 통신이 불안정했습니다. ROS2는 기본적으로 DDS(Data Distribution Service) 위에서 동작하며 멀티캐스트를 사용하는데, 두 네트워크가 혼재되어 디스커버리(Discovery) 실패가 잦았습니다.
이를 해결하기 위해 **DDS 미들웨어 설정(CycloneDDS XML 등)**을 통해 ROS2 통신이 **123.x 대역의 네트워크 인터페이스만 사용하도록 화이트리스트(Whitelisting) 처리**를 했습니다. 이렇게 트래픽을 격리하여 통신 안정성을 확보했습니다."

### Q7. CCTV 서버가 내부망(0.x)에 있는데, 어떻게 클라이언트(브라우저)에서 영상을 볼 수 있나요?
**질문 의도:** 웹 서버의 프록시 역할 이해.

**모범 답안:**
"브라우저는 보안상(CORS, 사설 IP 접근 불가) 0.x 대역의 CCTV 서버에 직접 붙을 수 없습니다.
그래서 저는 **FastAPI 백엔드를 리버스 프록시(Reverse Proxy)로 활용**했습니다.
웹 클라이언트가 백엔드의 `/cctv/stream` 엔드포인트를 호출하면, 백엔드가 내부망(0.x)을 통해 CCTV 서버의 스트림을 가져와서(`httpx.stream`) 클라이언트에게 그대로 전달(Relay)해주는 방식입니다. 이렇게 하면 클라이언트는 CCTV의 IP를 몰라도 영상을 볼 수 있습니다."

### Q8. 엣지 디바이스에서 LLM(Phi-4)을 구동하기 위해 어떤 최적화를 하셨나요?
**질문 의도:** 리소스 제약 환경에서의 모델 경량화 및 파인튜닝 경험 확인.

**모범 답안:**
"노트북 VRAM이 8GB로 제한적이어서 처음 7B 모델을 시도했을 때는 4-bit 양자화와 CPU Offloading을 해도 메모리 부족(OOM)이 발생했습니다.
그래서 **Phi-4 (3B)** 급의 소형 모델(SLM)로 전환했고, 여기서도 **4-bit 양자화(Quantization)**를 적용하여 겨우 메모리에 적재할 수 있었습니다.
파인튜닝 시에는 전체 파라미터를 학습시키는 것은 불가능하여, **LoRA(Low-Rank Adaptation)** 기술을 사용했습니다. 이는 마치 양파 껍질처럼 얇은 어댑터 레이어만 추가하여 학습시키는 방식으로, 적은 VRAM으로도 효율적인 학습이 가능했습니다.
또한, 작은 모델의 추론 능력을 극대화하기 위해 'Textbook Quality' 원칙에 따른 **Chain-of-Thought(CoT) 프롬프트 엔지니어링**을 적용하여 논리적 추론 능력을 보완했습니다."

### Q9. YOLO 모델 개발 과정에서 가장 어려웠던 점은 무엇인가요?
**질문 의도:** 데이터 엔지니어링 및 MLOps 경험 확인.

**모범 답안:**
"모델 아키텍처 자체보다는 **데이터 파이프라인 구축**이 가장 힘들었습니다.
실시간성을 위해 가벼운 YOLO Nano(n) 모델을 선택하여 추론 속도는 확보했지만, 정확도를 높이는 것이 관건이었습니다.
단순히 대상(사람) 사진만 모으는 것이 아니라, 오탐지(False Positive)를 줄이기 위해 다양한 '배경(Background)' 이미지와 '다른 사람(Negative Sample)' 데이터를 수집하고 라벨링하는 전처리 과정에 전체 시간의 80% 이상을 쏟았습니다. 깨끗한 데이터셋(Data Cleaning)이 모델 튜닝보다 훨씬 중요하다는 것을 깨달았습니다."

### Q10. 1인 개발을 하면서 가장 힘들었던 점과 이를 어떻게 극복했나요? (Behavioral & Soft Skills)
**질문 의도:** 자기 주도적 학습 능력, 끈기, 그리고 풀스택 개발자로서의 경험 확인.

**모범 답안:**
"**'두려움'과 'UI/UX의 디테일'**이 가장 힘들었습니다.
첫째, AI가 제안하는 낯선 기술(ROS2, 양자화 등)을 적용할 때 '내가 이걸 이해하고 컨트롤할 수 있을까?' 하는 두려움이 있었습니다. 이를 극복하기 위해 **철저한 버전 관리(Git) 원칙**을 세웠습니다. 새로운 시도를 하기 전에는 반드시 커밋을 남겨 '실패해도 돌아갈 곳'을 만들어두고 과감하게 도전했고, 단순히 코드를 붙여넣기보다 원리를 이해하려고 공식 문서를 파고들었습니다.
둘째, 대시보드 UI를 반응형(Mobile/Desktop)으로 만들면서 CSS 그리드 배치와 사용성 간의 트레이드오프를 맞추는 게 백엔드 로직보다 더 힘들었습니다. 하지만 이 과정을 통해 사용자가 느끼는 '직관성'이 기술적 완성도만큼 중요하다는 것을 배웠습니다. 혼자서 리모트로 작업하다 보니 나태해질 수 있는 순간마다 1일 1커밋을 목표로 스스로를 다잡으며 프로젝트를 완주했습니다."

### Q11. FastAPI(AsyncIO)와 ROS2(Blocking)는 동작 방식이 다른데, 어떻게 하나의 프로세스에서 공존시켰나요? (Advanced Concurrency)
**질문 의도:** 비동기 프로그래밍과 스레딩 모델에 대한 심도 있는 이해 확인. (가장 기술적인 질문 중 하나)

**모범 답안:**
"네, 개념을 정확히 이해하고 적용했습니다.
**FastAPI**는 '비동기(Async)'라서 요청이 올 때까지 기다리지 않고 다른 일을 처리할 수 있습니다.
반면 **ROS2의 `rclpy.spin()` 함수**는 호출되는 순간 무한 루프(`while True`)에 빠져서 로봇 신호만 기다리는 **'블로킹(Blocking)'** 상태가 됩니다.
만약 메인 코드에서 그냥 `spin()`을 실행하면 웹 서버가 멈춰버립니다.
그래서 저는 **ROS2 실행부를 별도의 '스레드(Thread)'로 분리**했습니다. 이렇게 하면 웹 서버용 스레드와 로봇 제어용 스레드가 동시에 돌아가면서, 웹 서버도 안 멈추고 로봇도 계속 제어할 수 있게 됩니다."

### Q12. 영상 스트리밍과 제어 신호를 동시에 처리할 때 Latency(지연) 관리는 어떻게 했나요? (Protocol Optimization)
**질문 의도:** 웹소켓과 HTTP 스트리밍의 차이점 및 프로토콜 최적화 경험 확인.

**모범 답안:**
"모든 데이터를 웹소켓으로 처리하면 대용량 이미지 데이터 때문에 제어 패킷이 지연될 수 있습니다.
그래서 프로토콜을 용도에 맞게 분리했습니다.
1.  **영상(High Bandwidth):** 오버헤드가 적고 끊김에 강한 **HTTP MJPEG Streaming (`multipart/x-mixed-replace`)** 방식을 사용하여 별도 채널로 뚫었습니다.
2.  **제어/상태(Low Latency):** 실시간성이 중요한 로봇 조종(Cmd_vel)과 상태 값은 **WebSocket (`rosbridge`)**을 사용하여 양방향으로 빠르게 주고받았습니다.
이렇게 '고대역폭 데이터'와 '저지연 데이터'의 전송 방식을 분리함으로써 쾌적한 조종 경험을 제공할 수 있었습니다."

---

# Part 2: Technical Deep Dive (Advanced)

## 1. Web Frontend & UI/UX

### Q1. (UI/UX) 영문 텍스트 위에 마우스를 올리면 뒷면의 한글이 손전등처럼 비치는 효과는 어떻게 구현했나요?
**질문 의도:** Canvas API를 썼는지, CSS Masking을 썼는지, 그리고 DOM 조작 비용을 어떻게 관리했는지 묻는 질문입니다. (웹/프론트엔드 역량 검증)

**모범 답안:**
"Canvas를 사용하지 않고, **CSS Mask-Image** 속성과 **GSAP(GreenSock)** 라이브러리를 사용해 구현했습니다.
구체적으로는 똑같은 텍스트를 가진 두 개의 레이어(`layer-front`, `layer-back`)를 겹쳐두고,
**뒷면 레이어(Layer-Back)**에 `mask-image`로 구멍이 뚫린 커서 이미지를 적용했습니다.
그리고 자바스크립트로 마우스 좌표(`mouseX`, `mouseY`)를 실시간으로 추적하여 CSS 변수(`--x`, `--y`)만 업데이트하는 방식으로, 렌더링 부하를 최소화하면서 부드러운 '손전등(Flashlight)' 효과를 만들었습니다."

---

## 2. Robotics & Real-time Communication

### Q2. 로봇의 센서 데이터(Camera, TF)가 대용량인데, 웹 대시보드에서 지연(Latency) 없이 어떻게 처리하셨나요?
**질문 의도:** 대역폭(Bandwidth) 관리와 데이터 압축 경험, 그리고 ROS2의 QoS/Throttle 개념을 아는지 묻는 질문입니다.

**모범 답안:**
"가장 큰 병목은 역시 **카메라 영상 데이터**였습니다. 초기에 `Raw Image` 그대로 전송하니 640x480 해상도임에도 대역폭을 모두 차지해, 위치 정보(TF) 수신까지 지연되어 맵이 끊기는 현상이 발생했습니다.
이를 해결하기 위해 두 가지 최적화를 수행했습니다.
첫째, **이미지 압축(Compression)**입니다. `Raw` 대신 `CompressedImage` (JPEG) 포맷을 사용하여 데이터 크기를 **90% 이상** 줄였습니다.
둘째, **프로토콜 분리**입니다. 실시간성이 중요한 영상은 **WebRTC/MJPEG 스트리밍**으로 별도 채널을 뚫고, TF나 오동메트리 같은 가벼운 데이터만 **WebSocket(rosbridge)**으로 전송하여 서로 간섭하지 않게 만들었습니다."

## 3. On-device AI & Optimization

### Q3. VRAM 8GB 노트북에서 어떻게 3B LLM을 학습(Fine-tuning)까지 시켰나요? (OOM 해결 전략)
**질문 의도:** 단순 추론이 아닌 '학습' 과정에서의 메모리 최적화 기술(QLoRA, Gradient Accumulation, CPU Offloading)을 구체적으로 아는지 묻는 질문입니다.

**모범 답안:**
"학습 시 메모리 부족(OOM)이 가장 큰 문제였습니다. 이를 해결하기 위해 **4단계 최적화 전략**을 사용했습니다.
첫째, **QLoRA (Quantized LoRA)**를 적용해 모델을 **4-bit(NF4)**로 양자화하여 VRAM 사용량을 1/4로 줄였습니다.
둘째, **CPU Offloading**(`llm_int8_enable_fp32_cpu_offload=True`)을 켜서 VRAM이 부족할 때 시스템 RAM(64GB)을 빌려 쓰도록 설정했습니다.
셋째, **배치 크기(Batch Size)를 1로 줄이는 대신, Gradient Accumulation을 8로 설정**하여 메모리는 적게 쓰면서도 학습 안정성을 확보했습니다.
마지막으로, **Paged AdamW 8-bit 옵티마이저**를 사용하여 옵티마이저가 차지하는 메모리까지 극한으로 줄였습니다."

---

---



---

## 3. Technology Deep Dive (기술 심층 분석)

### 왜 WebSocket(Rosbridge)인가?
*   **기존 기술의 한계:** HTTP(REST API)는 클라이언트가 요청을 보내야만 응답을 주는 단방향 통신입니다. 로봇의 위치나 센서 데이터처럼 초당 수십 번 변하는 데이터를 HTTP로 요청(Polling)하면 서버 부하가 심하고 딜레이가 발생합니다.
*   **해결책:** WebSocket은 양방향 통신 채널을 열어두므로, 로봇이 데이터가 변할 때마다 즉시 클라이언트(웹)로 전송할 수 있습니다. `rosbridge`는 ROS 토픽을 JSON 형태로 웹소켓을 통해 래핑해주는 역할을 합니다.

### 왜 FastAPI인가? (vs Flask/Django)
*   **기존 기술의 한계:** Flask는 동기(Synchronous) 방식 기반이라, 긴 작업(Lidar 데이터 처리 등)이 수행되는 동안 다른 요청이 블로킹될 수 있습니다.
*   **해결책:** **FastAPI**는 Python의 `async/await`를 기본 지원하는 ASGI 프레임워크입니다. 이를 통해 I/O 바운드 작업(DB 조회, 외부 API 호출, 이미지 스트리밍)을 비동기로 처리하여 동시 접속자 처리에 훨씬 유리합니다. 또한 Pydantic을 이용한 데이터 검증이 강력하여 로봇 명령 데이터의 안정성을 높여줍니다.

### 왜 PHi-4인가? (Small Language Models - SLM)
*   **트렌드:** 거대 언어 모델(LLM)은 성능은 좋지만 막대한 연산 자원이 필요합니다. 최근에는 특정 도메인 성능은 유지하면서 파라미터 수를 줄인 SLM(Phi, Gemma, Llama-3-8B)이 로보틱스 분야에서 주목받고 있습니다.
*   **이유:** 로봇의 제한된 컴퓨팅 파워(NVIDIA Jetson 등)에서도 추론이 가능해야 하기 때문입니다.

---

## 4. Problem Solving Stories (문제 해결 사례)

### 사례 1: 환경 변수 관리와 보안
*   **문제:** API 키와 네트워크 포트 정보가 코드에 하드코딩되어 있어 보안 위험이 있었고, 배포 환경(집, 학교 등)이 바뀔 때마다 코드를 수정해야 했음.
*   **해결:** `.env` 파일을 도입하여 `dotenv` 라이브러리로 관리하도록 리팩토링함. 이를 통해 코드는 건드리지 않고 환경 변수만 변경하여 다양한 네트워크 환경에 유연하게 대처할 수 있게 됨.

### 사례 2: ROS2와 웹 좌표계 동기화
*   **문제:** ROS2의 좌표계(중앙이 0,0)와 웹 캔버스의 좌표계(좌상단이 0,0)가 달라 맵이 뒤집혀 보이거나 클릭한 위치로 로봇이 가지 않는 문제 발생.
*   **해결:** Affine 변환(이동, 회전, 스케일) 행렬을 적용하여 좌표계를 매핑함. `roslibjs`의 GridMap 정보를 파싱하여 resolution(해상도)과 origin(원점) 데이터를 이용해 정확한 픽셀-미터 변환 공식을 구현함.

---

### 사례 3: 이중 네트워크 대역(Dual Network Interface) 처리
*   **문제:** 로봇(123.x)과 CCTV(0.x)가 서로 다른 내부망 대역을 사용하고 있어, 단순히 연결만 해서는 ROS2 통신(멀티캐스트)이 제대로 동작하지 않는 문제 발생. ROS2는 기본적으로 모든 네트워크 인터페이스로 패킷을 쏘는데, 0.x 대역으로 나간 패킷은 로봇에 도달하지 못해 통신 불안정 야기.
*   **해결:** **DDS(Data Distribution Service) 미들웨어 설정**을 통해 해결함. (사용자가 'mms'라고 기억한 기술은 아마도 DDS일 것임). 구체적으로는 `CYCLONEDDS_URI` 또는 XML 설정을 통해 ROS2 통신이 **123.x 대역의 네트워크 인터페이스(WLAN/Ethernet)만 사용하도록 바인딩(Interface Binding)**하여 트래픽을 격리하고 통신 성공률을 100%로 끌어올림.

### 사례 4: CCTV 외부망 연동 (Reverse Proxy Pattern)
*   **문제:** CCTV 카메라는 0.x 내부망에 있어, 외부(또는 다른 대역)에 있는 웹 브라우저가 직접 `http://192.168.0.197`에 접속하여 영상을 볼 수 없음 (CORS 및 라우팅 문제).
*   **해결:** FastAPI 백엔드에 **리버스 프록시(Reverse Proxy)** 엔드포인트를 구현함. 웹 클라이언트가 백엔드(`/cctv/stream`)로 요청을 보내면, 백엔드가 서버 내부 통신망을 통해 CCTV 서버에서 스트림을 받아와 클라이언트에게 중계(Relay)해주는 방식. 이를 통해 클라이언트는 CCTV IP를 알 필요가 없고, 보안성도 강화됨.

### 사례 5: 펫 모드의 센서 퓨전 (Vision + LiDAR)
*   **문제:** 카메라(YOLO)만으로는 대상과의 정확한 '거리'를 알기 어렵고, 라이다(LiDAR)만으로는 장애물이 '사람'인지 알 수 없음.
*   **해결:** YOLO로 인식한 사람의 Bounding Box 중심각을 구하고, 그 각도에 해당하는 LiDAR의 레이저 스캔 데이터를 매핑하는 **Sensor Fusion** 알고리즘을 구현함. 이를 통해 "주인(Owner)과의 거리 40cm 유지"라는 정밀한 제어가 가능해짐.
