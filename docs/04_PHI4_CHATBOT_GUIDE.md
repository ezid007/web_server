# PHi-4 ì±—ë´‡ ì‚¬ìš© ì„¤ëª…ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [í´ë” êµ¬ì¡°](#í´ë”-êµ¬ì¡°)
3. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
4. [í•™ìŠµ ë°ì´í„° ì¤€ë¹„](#í•™ìŠµ-ë°ì´í„°-ì¤€ë¹„)
5. [ë°ì´í„°ì…‹ ìƒì„±](#ë°ì´í„°ì…‹-ìƒì„±)
6. [ëª¨ë¸ í•™ìŠµ](#ëª¨ë¸-í•™ìŠµ)
7. [í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©](#í•™ìŠµëœ-ëª¨ë¸-ì‚¬ìš©)
8. [ì›¹ í†µí•©](#ì›¹-í†µí•©)
9. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ê°œìš”

PHi-4ëŠ” **Phi-4 ë…¼ë¬¸**ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•œ í•œêµ­ì–´ ì¶”ë¡  AI ì±—ë´‡ì…ë‹ˆë‹¤.

### í•µì‹¬ ì›ë¦¬
- **Synthetic Data Generation**: ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ê³ í’ˆì§ˆ ì¶”ë¡  ë¬¸ì œë¡œ ë³€í™˜
- **Chain-of-Thought (CoT)**: ë‹¨ê³„ë³„ ë…¼ë¦¬ì  ì‚¬ê³  í›ˆë ¨
- **LoRA Fine-tuning**: ì ì€ ë©”ëª¨ë¦¬ë¡œ ëŒ€í˜• ëª¨ë¸ í•™ìŠµ

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
| í•­ëª© | ìµœì†Œ ì‚¬ì–‘ | ê¶Œì¥ ì‚¬ì–‘ |
|------|----------|----------|
| GPU | 8GB VRAM | 12GB+ VRAM |
| RAM | 16GB | 32GB+ |
| ì €ì¥ê³µê°„ | 20GB | 50GB+ |

---

## í´ë” êµ¬ì¡°

```
Phi-4/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ seeds/              # ì›ì‹œ í•™ìŠµ ë°ì´í„° (í…ìŠ¤íŠ¸ íŒŒì¼)
â”‚   â”‚   â”œâ”€â”€ medical/        # ì˜ì•½í’ˆ ì •ë³´ í…ìŠ¤íŠ¸
â”‚   â”‚   â””â”€â”€ example.txt     # ì‹œë“œ í…ìŠ¤íŠ¸ ì˜ˆì‹œ
â”‚   â””â”€â”€ synthetic/
â”‚       â””â”€â”€ train.jsonl     # ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ì…‹
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/           # ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸
â”‚   â”‚   â”œâ”€â”€ generate_dataset.py   # ë°ì´í„°ì…‹ ìƒì„± ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚   â”œâ”€â”€ generator.py          # Ollama ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±ê¸°
â”‚   â”‚   â””â”€â”€ schemas.py            # Pydantic ë°ì´í„° ìŠ¤í‚¤ë§ˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ train/              # ëª¨ë¸ í•™ìŠµ
â”‚   â”‚   â””â”€â”€ train.py        # SFT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚
â”‚   â”œâ”€â”€ test/               # í…ŒìŠ¤íŠ¸
â”‚   â”‚   â””â”€â”€ inference.py    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ inspect_data.py # ë°ì´í„° ê²€ì‚¬ ë„êµ¬
â”‚
â””â”€â”€ models/                 # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ (git ì œì™¸)
    â””â”€â”€ phi-4-3b-reasoning/ # LoRA ì–´ëŒ‘í„°
```

---

## í™˜ê²½ ì„¤ì •

### 1. Ollama ì„¤ì¹˜ (ë°ì´í„° ìƒì„±ìš©)

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.ai/install.sh | sh

# ë°ì´í„° ìƒì„±ìš© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen2.5:7b
```

### 2. Python ì˜ì¡´ì„±

í”„ë¡œì íŠ¸ ë£¨íŠ¸(`web_server/`)ì—ì„œ:

```bash
poetry install
```

ì£¼ìš” ì˜ì¡´ì„±:
- `torch>=2.0.0` - PyTorch
- `transformers>=4.40.0` - Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸
- `peft>=0.10.0` - LoRA í•™ìŠµ
- `bitsandbytes>=0.43.0` - 4ë¹„íŠ¸ ì–‘ìí™”
- `accelerate>=0.28.0` - GPU ìµœì í™”
- `langchain-ollama` - Ollama ì—°ë™

---

## í•™ìŠµ ë°ì´í„° ì¤€ë¹„

### ì‹œë“œ í…ìŠ¤íŠ¸ í˜•ì‹

`data/seeds/` í´ë”ì— `.txt` íŒŒì¼ë¡œ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ: `data/seeds/medical/aspirin.txt`**

```text
ì•„ìŠ¤í”¼ë¦°(Aspirin)ì€ ì•„ì„¸í‹¸ì‚´ë¦¬ì‹¤ì‚°(acetylsalicylic acid)ì˜ ìƒí’ˆëª…ìœ¼ë¡œ,
í•´ì—´, ì§„í†µ, í•­ì—¼ì¦ íš¨ê³¼ê°€ ìˆëŠ” ë¹„ìŠ¤í…Œë¡œì´ë“œ í•­ì—¼ì¦ì œ(NSAID)ì…ë‹ˆë‹¤.

ì‘ìš© ì›ë¦¬:
ì•„ìŠ¤í”¼ë¦°ì€ ì‚¬ì´í´ë¡œì˜¥ì‹œê²Œë‚˜ì œ(COX) íš¨ì†Œë¥¼ ë¹„ê°€ì—­ì ìœ¼ë¡œ ì–µì œí•˜ì—¬
í”„ë¡œìŠ¤íƒ€ê¸€ë€ë”˜ì˜ í•©ì„±ì„ ì°¨ë‹¨í•©ë‹ˆë‹¤.

ì ì‘ì¦:
- ë‘í†µ, ì¹˜í†µ, ê·¼ìœ¡í†µ
- ë°œì—´ ê°ì†Œ
- í˜ˆì „ ì˜ˆë°© (ì €ìš©ëŸ‰)

ì£¼ì˜ì‚¬í•­:
- ìœ„ì¥ê´€ ì¶œí˜ˆ ìœ„í—˜
- 12ì„¸ ë¯¸ë§Œ ì–´ë¦°ì´ ì‚¬ìš© ê¸ˆì§€ (ë ˆì´ ì¦í›„êµ°)
- ì„ì‹  3ê¸° ì‚¬ìš© ê¸ˆì§€
```

### ì¢‹ì€ ì‹œë“œ í…ìŠ¤íŠ¸ ì‘ì„± ê°€ì´ë“œ

| âœ… ê¶Œì¥ | âŒ í”¼í•´ì•¼ í•  ê²ƒ |
|--------|---------------|
| êµ¬ì²´ì ì¸ ì‚¬ì‹¤ í¬í•¨ | ëª¨í˜¸í•œ ì¼ë°˜ë¡  |
| ë…¼ë¦¬ì  ì¸ê³¼ê´€ê³„ | ë‹¨ìˆœ ë‚˜ì—´ |
| ì „ë¬¸ ìš©ì–´ + ì„¤ëª… | ì•½ì–´ë§Œ ì‚¬ìš© |
| 500~2000ì | ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í…ìŠ¤íŠ¸ |

---

## ë°ì´í„°ì…‹ ìƒì„±

### 1. Ollama ì„œë²„ ì‹¤í–‰ í™•ì¸

```bash
ollama serve  # ë³„ë„ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
```

### 2. í•©ì„± ë°ì´í„° ìƒì„±

```bash
cd Phi-4
python -m src.pipeline.generate_dataset
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“‚ Found 15 seed files in 'data/seeds'
ğŸš€ Starting batch generation... (This may take time)
[1/15] Processing: aspirin.txt... âœ… Done
[2/15] Processing: ibuprofen.txt... âœ… Done
...
ğŸ‰ Generation Complete!
ğŸ“Š Success Rate: 14/15
ğŸ’¾ Saved to: data/synthetic/train.jsonl
```

### 3. ìƒì„±ëœ ë°ì´í„° í™•ì¸

```bash
python -m src.utils.inspect_data
```

**ìƒì„±ë˜ëŠ” JSONL í˜•ì‹:**
```json
{
  "question": "ì•„ìŠ¤í”¼ë¦°ì´ í˜ˆì „ì„ ì˜ˆë°©í•˜ëŠ” ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
  "reasoning_steps": [
    "ì•„ìŠ¤í”¼ë¦°ì€ COX íš¨ì†Œë¥¼ ì–µì œí•©ë‹ˆë‹¤.",
    "COX ì–µì œë¡œ íŠ¸ë¡¬ë³µì‚° A2 ìƒì„±ì´ ê°ì†Œí•©ë‹ˆë‹¤.",
    "íŠ¸ë¡¬ë³µì‚° A2ëŠ” í˜ˆì†ŒíŒ ì‘ì§‘ì„ ì´‰ì§„í•˜ëŠ” ë¬¼ì§ˆì…ë‹ˆë‹¤.",
    "ë”°ë¼ì„œ í˜ˆì†ŒíŒ ì‘ì§‘ì´ ì–µì œë˜ì–´ í˜ˆì „ í˜•ì„±ì´ ì¤„ì–´ë“­ë‹ˆë‹¤."
  ],
  "answer": "ì•„ìŠ¤í”¼ë¦°ì€ COX íš¨ì†Œë¥¼ ë¹„ê°€ì—­ì ìœ¼ë¡œ ì–µì œí•˜ì—¬ í˜ˆì†ŒíŒì˜ íŠ¸ë¡¬ë³µì‚° A2 ìƒì„±ì„ ì°¨ë‹¨í•˜ê³ , ì´ë¥¼ í†µí•´ í˜ˆì†ŒíŒ ì‘ì§‘ì„ ì–µì œí•˜ì—¬ í˜ˆì „ í˜•ì„±ì„ ì˜ˆë°©í•©ë‹ˆë‹¤."
}
```

---

## ëª¨ë¸ í•™ìŠµ

### 1. í•™ìŠµ ì‹¤í–‰

```bash
cd Phi-4
python -m src.train.train
```

### 2. ì£¼ìš” ì„¤ì • (train.py)

```python
# ê¸°ë³¸ ëª¨ë¸ (3B íŒŒë¼ë¯¸í„°)
model_name = "Qwen/Qwen2.5-3B-Instruct"

# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
per_device_train_batch_size = 1   # ë°°ì¹˜ í¬ê¸° 1 (ì•ˆì „)
gradient_accumulation_steps = 8   # ëˆ„ì  ë°°ì¹˜ 8
max_length = 512                  # ìµœëŒ€ í† í° ê¸¸ì´

# LoRA ì„¤ì •
r = 16            # ì–´ëŒ‘í„° ë­í¬
lora_alpha = 32   # í•™ìŠµ ë°˜ì˜ë¥ 
```

### 3. í•™ìŠµ ì‹œê°„ ì˜ˆìƒ

| ë°ì´í„° ìˆ˜ | GPU | ì˜ˆìƒ ì‹œê°„ |
|----------|-----|----------|
| 100ê°œ | RTX 3070 8GB | 1-2ì‹œê°„ |
| 500ê°œ | RTX 3070 8GB | 5-8ì‹œê°„ |
| 1000ê°œ | RTX 4090 24GB | 3-4ì‹œê°„ |

### 4. í•™ìŠµ ê²°ê³¼

í•™ìŠµ ì™„ë£Œ ì‹œ `models/phi-4-3b-reasoning/` í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤:
- `adapter_config.json` - LoRA ì„¤ì •
- `adapter_model.safetensors` - í•™ìŠµëœ ê°€ì¤‘ì¹˜

---

## í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©

### ì¶”ë¡  í…ŒìŠ¤íŠ¸

```bash
cd Phi-4
python -m src.test.inference
```

### Python ì½”ë“œì—ì„œ ì‚¬ìš©

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    quantization_config=bnb_config,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")

# í•™ìŠµëœ ì–´ëŒ‘í„° í•©ì²´
model = PeftModel.from_pretrained(base_model, "models/phi-4-3b-reasoning")

# ì¶”ë¡ 
prompt = "<|im_start|>user\nì•„ìŠ¤í”¼ë¦°ì˜ ì‘ìš© ì›ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.<|im_end|>\n<|im_start|>assistant\n"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=512)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ì›¹ í†µí•©

### í˜„ì¬ êµ¬ì¡°

í•™ìŠµëœ ëª¨ë¸ì€ ì›¹ ì„œë²„ì™€ ë‹¤ìŒê³¼ ê°™ì´ í†µí•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```
web_server/
â”œâ”€â”€ models/phi-4-reasoning/    # í•™ìŠµëœ ì–´ëŒ‘í„° (from Phi-4/models/)
â”œâ”€â”€ src/chatbot.py             # ì±—ë´‡ API ëª¨ë“ˆ
â””â”€â”€ main.py                    # FastAPI ì„œë²„ (ì±—ë´‡ ë¼ìš°í„° í¬í•¨)
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì— ì¶”ê°€:
```
PHI4_ADAPTER_PATH=phi-4-reasoning
```

### API ì—”ë“œí¬ì¸íŠ¸

| ë©”ì„œë“œ | ê²½ë¡œ | ì„¤ëª… |
|--------|------|------|
| POST | `/api/chat` | ì±—ë´‡ ëŒ€í™” |
| GET | `/api/chat/status` | ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸ |

---

## ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ì—ëŸ¬

```bash
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**í•´ê²°ì±…:**
1. `train.py`ì—ì„œ `per_device_train_batch_size=1` í™•ì¸
2. `tokenizer.model_max_length = 512` (ë” ì§§ê²Œ ì„¤ì •)
3. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ: `nvidia-smi` í™•ì¸

### Ollama ì—°ê²° ì‹¤íŒ¨

```bash
ConnectionError: Failed to connect to Ollama
```

**í•´ê²°ì±…:**
```bash
# Ollama ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags

# ì„œë²„ ì¬ì‹œì‘
sudo systemctl restart ollama
```

### ì±—ë´‡ ì‘ë‹µ í’ˆì§ˆ ì €í•˜

ëª¨ë¸ì´ ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì§€ ì•ŠëŠ” ê²½ìš°:

1. **ì‹œë“œ ë°ì´í„° í’ˆì§ˆ ê°œì„ **
   - ë” êµ¬ì²´ì ì´ê³  ë…¼ë¦¬ì ì¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
   
2. **í”„ë¡¬í”„íŠ¸ í˜•ì‹ í™•ì¸**
   - ChatML í˜•ì‹ (`<|im_start|>`, `<|im_end|>`) ì‚¬ìš©

3. **í•™ìŠµ ë°ì´í„° ì–‘ ì¦ê°€**
   - ìµœì†Œ 100ê°œ ì´ìƒì˜ ê³ í’ˆì§ˆ ì‹œë“œ ê¶Œì¥

---

## ì°¸ê³  ìë£Œ

- [Phi-4 Technical Report (Microsoft)](https://arxiv.org/abs/2412.08905)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft)
