import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel


def main():
    # ==========================================
    # 1. ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    # ==========================================
    base_model_name = "Qwen/Qwen2.5-3B-Instruct"  # ì›ë³¸(í•™ìƒ) ëª¨ë¸
    adapter_path = "models/phi-4-3b-reasoning"  # ìš°ë¦¬ê°€ ê°€ë¥´ì¹œ ì§€ì‹(ì–´ëŒ‘í„°)

    print(f"ğŸ”„ Loading Base Model: {base_model_name}")

    # ==========================================
    # 2. 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
    # ==========================================
    # í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì´ 4ë¹„íŠ¸ë¡œ ë¶ˆëŸ¬ì™€ì•¼ 8GB VRAMì—ì„œ ë•ë‹ˆë‹¤.
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
    )

    # ==========================================
    # 3. ëª¨ë¸ ë¡œë”© ë° ë³‘í•©
    # ==========================================
    # 1) ì›ë³¸ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, quantization_config=bnb_config, device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)

    # 2) í•™ìŠµëœ ì–´ëŒ‘í„°(LoRA) ì¥ì°©
    # ì›ë³¸ ëª¨ë¸ ì˜†ì— ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¨ ì–‡ì€ ì§€ì‹ì¸µì„ ë¼ì›Œ ë„£ìŠµë‹ˆë‹¤.
    print(f"â• Merging Adapter: {adapter_path}")
    model = PeftModel.from_pretrained(base_model, adapter_path)

    # ==========================================
    # 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ ì§ˆë¬¸)
    # ==========================================
    # í•™ìŠµ ë°ì´í„°ì—ëŠ” ì—†ì§€ë§Œ, ì•½ë¦¬í•™ì  ì§€ì‹ê³¼ ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ë˜ì ¸ë´…ë‹ˆë‹¤.
    test_question = (
        "A patient with hypertension is prescribed a calcium channel blocker. "
        "Explain how this drug works to lower blood pressure, specifically focusing on vascular smooth muscle cells. "
        "Also, predict what would happen if the patient develops severe acidosis."
    )

    # í•™ìŠµ ë•Œ ì¼ë˜ í”„ë¡¬í”„íŠ¸ í¬ë§·(ChatML)ì„ ê·¸ëŒ€ë¡œ ì§€ì¼œì•¼ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜µë‹ˆë‹¤.
    prompt = (
        f"<|im_start|>system\nYou are a helpful assistant capable of complex reasoning.<|im_end|>\n"
        f"<|im_start|>user\n{test_question}<|im_end|>\n"
        f"<|im_start|>assistant\nLet's think step by step.\n"
    )

    # í…ìŠ¤íŠ¸ -> í† í° ë³€í™˜
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    print("\nğŸ¤– Generating Answer...\n")
    print("=" * 50)

    # ìƒì„± ì‹œì‘
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,  # ìµœëŒ€ 512í† í°ê¹Œì§€ ìƒì„±
            temperature=0.7,  # ì°½ì˜ì„± (0.7 ì •ë„ê°€ ì ë‹¹)
            top_p=0.9,  # ë‹¤ì–‘í•œ í‘œí˜„ ì‚¬ìš©
            do_sample=True,  # í™•ë¥ ì  ìƒì„± ì‚¬ìš©
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    # í† í° -> í…ìŠ¤íŠ¸ ë³€í™˜ (ê²°ê³¼ ì¶œë ¥)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # í”„ë¡¬í”„íŠ¸ ë’·ë¶€ë¶„(ìƒì„±ëœ ë‹µë³€)ë§Œ ê¹”ë”í•˜ê²Œ ì¶œë ¥
    # (system, user ë¶€ë¶„ì€ ì˜ë¼ë‚´ê³  assistant ë‹µë³€ë§Œ ë³´ì—¬ì¤Œ)
    answer_start = generated_text.find("Let's think step by step.")
    final_answer = generated_text[answer_start:]

    print(final_answer)
    print("=" * 50)


if __name__ == "__main__":
    main()
