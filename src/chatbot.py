"""
PHi-4 ì±—ë´‡ ëª¨ë“ˆ
í•œêµ­ì–´ ì¶”ë¡  AI ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì±„íŒ… APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import torch
from pathlib import Path
from fastapi import APIRouter
from fastapi.responses import JSONResponse

# ì›¹ ê²€ìƒ‰ ë° ìœ„ì¹˜ ê°ì§€ ëª¨ë“ˆ import
from src.web_search import search_web
from src.location import get_location_from_ip_sync, needs_location_context

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ê²½ë¡œ ë¡œë“œ
_base_dir = Path(__file__).parent.parent
_models_dir = _base_dir / "models"

def _resolve_model_path(model_name: str, default: str) -> Path:
    """ëª¨ë¸ ê²½ë¡œ í•´ì„: íŒŒì¼ëª…ë§Œ ìˆìœ¼ë©´ models/ í´ë”ì—ì„œ ì°¾ìŒ"""
    path = Path(model_name) if model_name else Path(default)
    if path.is_absolute():
        return path
    if path.parent == Path("."):
        return _models_dir / path
    return _base_dir / path

PHI4_ADAPTER_PATH = _resolve_model_path(
    os.getenv("PHI4_ADAPTER_PATH", ""), "phi-4-reasoning"
)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„ ì–¸
chatbot_model = None
chatbot_tokenizer = None
chatbot_loaded = False

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/api", tags=["chatbot"])

# ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ í‚¤ì›Œë“œ ëª©ë¡
SEARCH_KEYWORDS = [
    "ë‚ ì”¨", "ì˜¤ëŠ˜", "ë‰´ìŠ¤", "ìµœì‹ ", "í˜„ì¬", "ì§€ê¸ˆ", "ì£¼ê°€", "í™˜ìœ¨",
    "ê²€ìƒ‰", "ì°¾ì•„", "ì•Œë ¤ì¤˜", "ëª‡ì‹œ", "ëª‡ë„", "ì–´ë””", "ëˆ„ê°€", "ì–¸ì œ",
    "ì‹¤ì‹œê°„", "ì†ë³´", "ê²½ê¸°", "ê²°ê³¼", "ìŠ¤ì½”ì–´", "ìˆœìœ„"
]


def needs_web_search(query: str) -> bool:
    """ì§ˆë¬¸ì´ ì›¹ ê²€ìƒ‰ì„ í•„ìš”ë¡œ í•˜ëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    return any(keyword in query for keyword in SEARCH_KEYWORDS)


async def load_chatbot_model():
    """PHi-4 ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œ)"""
    global chatbot_model, chatbot_tokenizer, chatbot_loaded
    
    if chatbot_loaded:
        return
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not PHI4_ADAPTER_PATH.exists():
        print(f"âš ï¸ PHi-4 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PHI4_ADAPTER_PATH}")
        return
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        from peft import PeftModel
        
        print("â³ PHi-4 ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1~2ë¶„ ì†Œìš”)")
        
        base_model_name = "Qwen/Qwen2.5-3B-Instruct"
        
        # 8GB VRAMì„ ìœ„í•œ 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name, quantization_config=bnb_config, device_map="auto"
        )
        chatbot_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # í•™ìŠµëœ ì–´ëŒ‘í„° í•©ì²´
        chatbot_model = PeftModel.from_pretrained(base_model, str(PHI4_ADAPTER_PATH))
        chatbot_loaded = True
        print("âœ… PHi-4 ëª¨ë¸ ë¡œë”© ì™„ë£Œ! í•œêµ­ì–´ ì¶”ë¡  ì¤€ë¹„ ë.")
        
    except Exception as e:
        print(f"âš ï¸ PHi-4 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")


async def unload_chatbot_model():
    """PHi-4 ëª¨ë¸ ì–¸ë¡œë“œ (ì„œë²„ ì¢…ë£Œ ì‹œ í˜¸ì¶œ)"""
    global chatbot_model, chatbot_tokenizer, chatbot_loaded
    
    if chatbot_model:
        del chatbot_model
        chatbot_model = None
    if chatbot_tokenizer:
        del chatbot_tokenizer
        chatbot_tokenizer = None
    
    chatbot_loaded = False
    torch.cuda.empty_cache()


from pydantic import BaseModel
from typing import List, Optional

class ChatMessage(BaseModel):
    sender: str
    text: str

class ChatRequest(BaseModel):
    prompt: str
    history: Optional[List[ChatMessage]] = None
    client_ip: Optional[str] = None

@router.post("/chat")
async def chat(request: ChatRequest):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ëª¨ë¸ì´ ì¶”ë¡ í•˜ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global chatbot_model, chatbot_tokenizer, chatbot_loaded
    
    if not chatbot_loaded:
        return JSONResponse(
            content={"response": "âš ï¸ ì±—ë´‡ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
            status_code=503
        )
    
    try:
        # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ í™•ì¸ ë° ê²€ìƒ‰ ìˆ˜í–‰
        search_context = ""
        searched_web = False
        user_location = ""
        
        if needs_web_search(request.prompt):
            # ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° IPë¡œ ìœ„ì¹˜ ê°ì§€
            if needs_location_context(request.prompt):
                user_location = get_location_from_ip_sync(request.client_ip)
            
            print(f"ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {request.prompt}" + (f" (ìœ„ì¹˜: {user_location})" if user_location else ""))
            search_results = search_web(request.prompt, user_location=user_location)
            
            if search_results and "ì˜¤ë¥˜" not in search_results:
                searched_web = True
                search_context = (
                    f"\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼]\n{search_results}\n"
                    "ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
                )
                print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ íšë“")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
        # Hallucination ë°©ì§€ë¥¼ ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ê°•ì œ
        if searched_web:
            system_prompt = (
                "<|im_start|>system\n"
                "ë‹¹ì‹ ì€ ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í•œêµ­ì–´ AI ë¹„ì„œì…ë‹ˆë‹¤.\n"
                "ì¤‘ìš”: ì•„ë˜ [ì›¹ ê²€ìƒ‰ ê²°ê³¼]ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
                "ê·œì¹™:\n"
                "1. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
                "2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‰½ê³  ê°„ë‹¨í•œ ì¼ìƒ ì–¸ì–´ë¡œ ìš”ì•½í•´ì„œ ì „ë‹¬í•˜ì„¸ìš”.\n"
                "3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” '~ë¡œ ë³´ì…ë‹ˆë‹¤', '~ë¼ê³  í•©ë‹ˆë‹¤'ì²˜ëŸ¼ í‘œí˜„í•˜ì„¸ìš”.\n"
                "4. URL, ì¶œì²˜, ì´ëª¨ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "5. í•µì‹¬ ì •ë³´ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
                f"{search_context}<|im_end|>\n"
            )
        else:
            system_prompt = (
                "<|im_start|>system\n"
                "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ AI ë¹„ì„œì…ë‹ˆë‹¤.\n"
                "ê·œì¹™:\n"
                "1. ì‰½ê³  ê°„ë‹¨í•œ ì¼ìƒ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
                "2. í™•ì‹¤íˆ ì•„ëŠ” ì •ë³´ë§Œ ë‹µë³€í•˜ê³ , ëª¨ë¥´ë©´ ì†”ì§íˆ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  í•˜ì„¸ìš”.\n"
                "3. ì‚¬ì‹¤ì´ ì•„ë‹Œ ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
                "4. ì´ëª¨ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.<|im_end|>\n"
            )
        
        # ëŒ€í™” ë‚´ì—­ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        history_prompt = ""
        if request.history:
            for msg in request.history:
                if msg.sender == "user":
                    history_prompt += f"<|im_start|>user\n{msg.text}<|im_end|>\n"
                elif msg.sender == "bot":
                    history_prompt += f"<|im_start|>assistant\n{msg.text}<|im_end|>\n"
        
        # í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
        user_prompt = f"<|im_start|>user\n{request.prompt}<|im_end|>\n"
        assistant_start = "<|im_start|>assistant\n"
        
        full_prompt = system_prompt + history_prompt + user_prompt + assistant_start
        
        inputs = chatbot_tokenizer(full_prompt, return_tensors="pt").to("cuda")
        
        # ì¶”ë¡  ìƒì„±
        with torch.no_grad():
            outputs = chatbot_model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                eos_token_id=chatbot_tokenizer.eos_token_id,
                pad_token_id=chatbot_tokenizer.eos_token_id,
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = chatbot_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ ì¶”ì¶œ (ë” ê°•ë ¥í•œ íŒŒì‹±)
        final_answer = generated_text
        
        # 1. ë§ˆì§€ë§‰ "assistant" ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "assistant" in final_answer:
            final_answer = final_answer.split("assistant")[-1].strip()
        
        # 2. "user" ë§ˆì»¤ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ê·¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ë‹¤ìŒ ëŒ€í™” ì‹œì‘ ì œê±°)
        if "\nuser" in final_answer:
            final_answer = final_answer.split("\nuser")[0].strip()
        if final_answer.startswith("user"):
            # ì²« ì¤„ì´ userë¡œ ì‹œì‘í•˜ë©´ ì œê±°
            lines = final_answer.split("\n")
            final_answer = "\n".join(lines[1:]).strip()
        
        # 3. ì‚¬ìš©ì ì§ˆë¬¸ì´ ë°˜ë³µë˜ë©´ ì œê±°
        if request.prompt in final_answer:
            final_answer = final_answer.replace(request.prompt, "").strip()
        
        # 4. ğŸ” ì´ëª¨ì§€ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì œê±° (ê²€ìƒ‰ ì•Œë¦¼ê³¼ ì¤‘ë³µ ë°©ì§€)
        lines = final_answer.split("\n")
        filtered_lines = [line for line in lines if not line.strip().startswith("ğŸ”")]
        final_answer = "\n".join(filtered_lines).strip()
        
        # 5. URL í¬í•¨ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        import re
        # URL ì œê±°
        final_answer = re.sub(r'https?://\S+', '', final_answer)
        # "~ì— ê°€ë©´", "~ì—ì„œ" ê°™ì€ ë¶ˆì™„ì „ ì‹œì‘ ì œê±°
        final_answer = re.sub(r'^[\s]*ì—\s+(ê°€ë©´|ë³´ë©´|í™•ì¸)', '', final_answer)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        final_answer = re.sub(r'\s+', ' ', final_answer).strip()
        # ë¹ˆ ì‘ë‹µì´ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not final_answer or len(final_answer) < 10:
            final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤, í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        
        return JSONResponse(content={
            "response": final_answer,
            "searched_web": searched_web
        })
        
    except Exception as e:
        print(f"âš ï¸ ì±—ë´‡ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        return JSONResponse(
            content={"response": f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"},
            status_code=500
        )


@router.get("/chat/status")
async def chat_status():
    """ì±—ë´‡ ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
    return JSONResponse(content={
        "loaded": chatbot_loaded,
        "model_path": str(PHI4_ADAPTER_PATH),
        "model_exists": PHI4_ADAPTER_PATH.exists()
    })
